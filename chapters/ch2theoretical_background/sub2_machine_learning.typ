#import "@preview/abbr:0.2.3"
== Deep Learning
Deep Learning has gained increasing popularity in recent years, particularly through advancements in #abbr.pla[NN]. These developments have significantly expanded the capabilities of automated data-driven modeling across various domains. In this chapter, we focus primarily on #abbr.pla[NN] architectures, as they form the core modeling approach used in this project.


=== Feedforward Neural Networks<fnn>
#abbr.pla[FNN] are a class of machine learning models inspired by the structure and function of the human brain. In biological systems, neurons are interconnected through synapses, and their strengths change in response to external stimuli—a process that underlies learning. #abbr.pla[FNN] mimic this behavior by using computational units, also called neurons, connected by weighted links. These weights are adjusted during training to improve the model's predictions, analogous to synaptic strength adjustments in the brain.

Each artificial neuron receives inputs, scales them using learned weights, applies a non-linear activation function, and forwards the result to subsequent neurons. Through this architecture, an #abbr.a[FNN] models complex functions by propagating signals from input to output layers. Learning in #abbr.pla[FNN] occurs via exposure to training data consisting of input–output pairs. The network adjusts its weights to reduce the difference between its predictions and the target outputs, thereby minimizing the prediction error.

While the biological analogy is imperfect, it has historically guided the development of neural architectures. More formally, #abbr.pla[FNN] can also be viewed as compositions of simple mathematical units—such as logistic or linear regressors—structured into a computational graph. Their expressive power arises from stacking these units into deeper networks, enabling them to approximate highly non-linear relationships in data. This capacity to learn from examples and generalize to unseen inputs makes #abbr.pla[FNN] a powerful tool in modern machine learning.


*Architecture*

An #abbr.a[FNN] trained with backpropagation, which is discussed in @backprop, can be illustrated as a directed acyclic Graph with inter-connections. It contains a set of neurons distributed in different layers.
- Each neuron has a activation function.
- The first layer, shown on the left side in @nn-img, is called the input layer and has no predacessors in the inter-connection graph. Additionally, is their input value the same as their output value.
- The last layer, shown on the right side in @nn-img, is called the output layer and have no successors in the inter-connection graph. Their value represents the output of the Network
- All other neurons are grouped in the so called hidden layers. In @nn-img this is represented by the layer in the middle. A neural network can have an arbitrary amount of hidden layers. 
- The edges in the inter-connection graph, are so called weights, which represent an arbitrary number in $RR$. These weights are updated during the trianing process.

#figure(image("images/Neural_Network_Illustration.png", width: 60%), 
caption: [Illustration of a Neural Network with 3 layers. Illustrated with @NNSVG])
<nn-img>

*Computation of the Output* 

The calculation of the output of the #abbr.a[FNN] is also called a forward pass. To do so, the value of each neuron needs to be calculated. This is done by summing all the inputs and then put this value into a given activation function. Mathematically, this process can be represented as: $y = f(sum_i^n xi_i)$ wher $f$ represents the activation function of the neuron and $xi_i$ the input or also called the potential of a neuron. To compute a full forward pass, this is done for each neuron from the input layer towards the output layer. When a value is passed throughtouh a weight to a successor neuron, the value is multiplied by the value of the weight. This process can then be summarized to:

#math.equation(block: true, supplement: auto, 
$
    y_1 = f(sum_i^n w_(i j) x_i) "[input to hidden layer]"\
    y_(j+1) = f(sum_i^n w_(i j) y_j)  forall j in {1 dots k-1}"[hidden to hidden layer]"\
    o = f(w_(i j+1) y_j) "[hidden to output layer]"
$ 
)<forwardpass> @aggarwalNeuralNetworksDeep2023
where $j$ denotes the layer, ascending from input layer to output layer, $f$ activation function, $w_(i j)$ weight at index $i$ and layer $j$, $x$ as input at index $i$. The state of the neuron in the output layer $o$ can then by denoted as the output vector.


=== The Backpropagation Training Algorithm<backprop>
*Objective*: To identify a set of weights that guarantees that for every input vector, the output vector generated by the network is identical to (or sufficiently close to) the desired output vector.

Note: The actual or desired output values of the hidden neurons are not explicitly specified by the task.

*For a fixed and finite training set*:
The objective function represents the total error between the desired and actual outputs of all the output neurons for all the training patterns.

*Error Function*

#math.equation(
  block: true,
  $
    E = 1 /2 sum_p^P sum_i^N (y_(i p) - d_(i p))^2 
  $
)<errorfunction> @mrazovaMultilayeredNeuralNetworks


where $P$ is the number of training patterns, $N$ the number of output neurons, $d_(i p)$ is the desired output for pattern $p$, $y_(i p)$ the actual output of the neuron $i$ and output neuron $i$.

*Procedure*
#figure(
  box[
    #set align(left)
    1. Compute the actual output for the presented pattern
    2. Compare the actual output with the desired output
    3. Adjustment of weights and thresholds against the gradient of the error function (@errorfunction) for each layer from the output layer towards the input layer
  ], 
  caption: [Training Procedure of the backpropagation algorithm]
)

*Adjustment Rules*

#math.equation(
  block: true,
  $
    w_(i j)(t + 1) = w_(i j) + Delta_E w_(i j) (t) \
    Delta_E w_(i j) = - (partial E) / (partial w_(i j)) = - (partial E) / (partial y_j) (partial y_j) / (partial xi_j) (partial xi_j) / (partial w_(i j))
  $
)#cite(<mrazovaMultilayeredNeuralNetworks>)

where $Delta_E w_(i j)$ denotes the change of the Error Function with respect to $w_(i j)$, $E$ the Error Function, $y_j$ the output of the output neuron $j$, $xi_j$ the potential of the neuron $j$ and $w_(i j)$ the weight with index $i$ at layer $j$.

=== Recurrent Neural Networks
The feedforward #abbr.pla[FNN] discussed in @fnn are inherently limited to fixed-size, unordered input representations. This makes them unsuitable for sequential data, where both the order and length of the input can vary. To address this limitation, we introduce a class of models specifically designed to process variable-length sequences: #abbr.pla[RNN]

*Architecture*

A #abbr.a[RNN] consits of the following components:
- Input signal: The external data which is fed into the network at a timestep $n$ and represent the current information which the network is processing.
- State signal: Also known as the hidden state, represents the memory of the #abbr.a[RNN] for a given neuron. It contains information about the past inputs in the sequence and is updated at each time step based on the current input and the previous state. The hidden state is updated with the following formula: $h_t = f(h_(t-1), x_t)$. After the update, the hidden state of neuron $i$ serves as input into the neuron $i+1$ 
- Weights: The weights of the #abbr.a[RNN] neurons are shared among all different states. 
- Output: Each neuron has a output, which is denoted as $y_1$ - $y_4$ in @fig:unrolled_rnn. This output can serve as the output for the current state or as input into the next neuron. 
@schillingLecture05Sequential2025, @aggarwalNeuralNetworksDeep2023

 #grid(
  columns: 2, align: center,
  grid.cell([
    #figure(
      image("images/rnn_simple.png", height: 25%),
      caption: [#abbr.a[RNN]],
    )<fig:rnn>
  ]),
  grid.cell([
    #figure(
      image("images/rnn_unrolled.png", height: 25%),
      caption: [4 times unrolled #abbr.a[RNN]],
    )<fig:unrolled_rnn>
  ]),
)


*Vanishing Gradient Problem*

The #abbr.a[VGP] is a challenge encountered during the training of of #abbr.pla[RNN], particullary dealing with deep #abbr.pla[RNN] and long input sequences. It arieses from the way how gradients are updated during the backpropagation algorithm (discussed in @backprop), which updates the network's paramertes / weights by propagating gradients backward through each time step. In backpropagation, gradients are computed via the chain rule, resulting in repeated multiplication of weight matrices and derivatives of activation functions across time steps. When these values are consistently smaller than one, the gradients exponentially decrease as they traverse earlier layers or time steps. Consequently, the gradients become vanishingly small, leading to negligible updates for earlier parameters and impairing the network's ability to learn long-range dependencies. The same principle aries, when the gradients become too large. In this case, the problem is called the exploding gradient problem.

@aggarwalNeuralNetworksDeep2023



=== Long Short Term Memory Neural Networks
#abbr.pla[LSTM] are a special form of #abbr.pla[RNN] designed to address the problem of Vanishing Gradients while having a more fine-grained control over the previous input data and were introduced for the first time by  Sepp Hochreiter in 1997 @hochreiterLongShortTermMemory1997. #abbr.pla[LSTM] are an enhanement of #abbr.pla[RNN], because the recurrence conditaions of how the hidden state $h_t$ is processed. To achieve this aim, we introduce a new hidden state of the same dimesion as $h_t$, which is called the cell state and is denoted as $c_t$. 
The key innovation of the #abbr.a[LSTM]  lies in its ability to control the flow of information using a set of gating mechanisms. These gates regulate how information is added to, removed from, or exposed from the cell state. Each gate is implemented as a sigmoid-activated neural layer and serves a distinct role in the update process.

*Architecture*

The internal structure of an #abbr.a[LSTM] cell is shown in @lstm-illustration. The figure illustrates how, at each time step $t$, the cell takes in the input vector $x_t$, the previous hidden state $h_(t-1)$, and the previous cell state $c_(t-1)$, and uses them to compute updated values for the current cell state $c_t$ and hidden state $h_t$.

@lstm-illustration shows an illustration of an #abbr.a[LSTM] Cell. 
#figure(
  image("images/LSTM_Cell_illustration.png"),
  caption: [
Schematic illustration of an #abbr.a[LSTM] cell highlighting the internal gating structure. The colored blocks represent the three core gates—Forget (blue), Input (green), and Output (red)—and show how they interact with the cell and hidden states to regulate information flow.
]
  )<lstm-illustration>

At each time step $t$ with a given input vector $x_t$, previous hidden state $h_(t-1)$ and previous cell state $c_(t-1)$, the #abbr.a[LSTM] performs the following computations:

- Forget Gate (shown in the blue part of @lstm-illustration): This gate decides which parts of the previous cell state should be forgotten. The value of the forget gate is calculated as: 
  - $f_t = sigma(w_f [h_(t-1), x_t]) + b_f) $
- Input Gate (shown in the green part of @lstm-illustration ): Decides which new information will be added to the cell state and is calculated as:
  - $i_t = sigma(w_i [h_(t-1), x_t] + b_i)$
- Output Gate (shown as the red part in @lstm-illustration): Determines which part of the cell state influences the hidden state and therefore the output. It is computed with: 
  - $o_t = sigma(w_o [h_(t-1), x_t] + b_o)$
- Candidate Cell State: Computes possible candidates $tilde(c_t)$ which can be added to the cell state, computed as: 
  - $tilde(c_t) = tanh(w_c [h_(t-1), x_t] + b_c)$
- Cell state update: Given the candidates, the cell state can be updated as:
  - $c_t = f_t dot c_(t-1) + i_t dot tilde(c_t)$
- Hidden state update:  The final hidden state is computed by applying the output gate to the activated cell state. It is computed with: 
  - $h_t = o_t dot tanh(c_t)$
Note: $w_x$ represents a complete weight matrix for each gate, $b_x$ denotes the bias for the corresponding gates, and $sigma$ denotes the sigmoid function.

@PyTorchFoundation, @thakurLSTMItsEquations2018  

=== Transformer
With the advent of Large Language Models and influential works such as Attention Is All You Need @vaswaniAttentionAllYou2023, Transformer architectures have gained significant traction in the field of machine learning. Originally developed for natural language processing tasks, Transformers have since been successfully adapted to a variety of domains, such as time series forcasting as shown by Q. Wen et. al. in  "Transformers in Time Series: A Survey" @wenTransformersTimeSeries2023 due to their ability to model long-range dependencies.

In the following section, the core components and mechanisms of the Transformer architecture are outlined. Furthermore, special emphasis is placed on its applicability to time series forecasting—a setting in which capturing temporal patterns and complex dependencies is crucial.

*Architecture*

An important concept in the Transformer architecture is Attention. It allows the model to capture dependencies between elements in the input sequence. An attention function can be viewed as a mapping from a query and a set of key–value pairs to an output. The output is a weighted sum of the values, where the weights are determined by a compatibility function between the query and the keys. This mechanism is illustrated in @self-attention-ill, where the input sequence is linearly projected into query, key, and value matrices to compute attention scores and generate contextualized representations. In tasks involving sequential data, such as language modeling or time series forecasting, the model should not have access to future positions when making a prediction. To enforce this constraint, the Transformer uses a technique called masked attention, in which the attention weights for all positions beyond the current one are set to zero. This ensures that, when computing the representation for position $x_n$, the model can only attend to $x_(<n)$ through $x_n$, but not to any $x_(>n)$.





#figure(image("images/self-attention.png"),
caption: [
Self-attention mechanism illustrated with matrices. All matrices have shape $D dot N$, where $D$ is the sequence length and $N$ is the feature dimension. The input matrix is projected into three separate matrices: Queries ($Q$), Keys ($K$), and Values ($V$). The attention weights are computed by multiplying $Q$ with the transpose of $K$, followed by the Softmax function. The result is then used to weight the $V$ matrix, producing the final output as $"Softmax"(Q K^T) dot V$.
@princeUnderstandingDeepLearning
]
)<self-attention-ill>

@vaswaniAttentionAllYou2023, @frank-peter09_Transformers2025, @princeUnderstandingDeepLearning

Transformers follow an encoder–decoder architecture, as illustrated in @encoder-decoder-architecture-ill. In this framework, the encoder processes the full input sequence and produces a contextual representation, called the Encoder Vector as shown in @encoder-decoder-architecture-ill in grey. The decoder uses the Encoder Vector to generate the output sequence token by token. While both encoder and decoder are composed of multiple stacked layers and share a similar modular structure, including feedforward sub-layers, skip connections, and normalization steps. The decoder includes additional mechanisms to ensure autoregressive generation, which refers to the process of generating output tokens one at a time and each token is generated based on all previous generated tokens.
#figure(
  image("images/encoder_decoder_architecture_illustration.png"),
  caption: [Abstracted illustration of the encoder–decoder architecture. The encoder receives an input sequence $x_1, dots, x_n$ and transforms it into a sequence of contextualized representations, here called the Encoder Vector, which is symbolically represented by two arrows to emphasize its role in guiding the decoding process. The encoder vector are passed to the decoder, which generates an output sequence $y_1, dots, y_k$, where the output length $k$ may differ from the input length $n$.@aggarwalNeuralNetworksDeep2023. 
]
)<encoder-decoder-architecture-ill>

Combining the encoder–decoder structure with the attention mechanism results in the full Transformer model. In this architecture, self-attention is used within both the encoder and decoder to enable each position in a sequence to access contextual information from all other positions. 


During training, the encoder receives the full observed input sequence, such as past weather patterns over several weeks in the domain of weather forcasting. The decoder is provided with the leftmost portion of the target sequence, which is, the known values from the beginning of the forecast window. For example, if the goal is to predict the temperature over the next 10 days, the decoder might initially receive only a start-of-sequence token or the first known value and must predict the next value in the sequence. To prevent the decoder from accessing future values during training, a masking strategy is applied in the self-attention, layers. This ensures that each prediction depends only on earlier positions in the output sequence, simulating real-world forecasting conditions. The model is trained by comparing each predicted value to the actual value using a suitable loss function such as cross-entropy or mean squared error, depending on the output type. This approach enables the model to learn autoregressive generation, where each future value is predicted step by step, conditioned on both the encoder input and previously predicted outputs. @aggarwalNeuralNetworksDeep2023


Although the original Transformer combines encoder and decoder modules, simplified variants such as BERT and GPT-3 omit either the decoder or encoder component. BERT uses an encoder-only architecture suited for classification and representation tasks, while GPT-3 is built on a decoder-only architecture optimized for generative tasks.
@frank-peter09_Transformers2025


