#import "@preview/abbr:0.2.3"
== Deep Learning
Deep Learning has gained increasing popularity in recent years, particularly through advancements in #abbr.pla[NN]. These developments have significantly expanded the capabilities of automated data-driven modeling across various domains. In this chapter, we focus primarily on #abbr.a[NN] architectures, as they form the core modeling approach used in this project.


=== #abbr.pla()[FNN]<fnn>
#abbr.a[FNN] are a class of machine learning models inspired by the structure and function of the human brain. In biological systems, neurons are interconnected through synapses, and their strengths change in response to external stimuli—a process that underlies learning. #abbr.a[FNN] mimic this behavior by using computational units, also called neurons, connected by weighted links.
*Architecture*

An #abbr.a[FNN] trained with backpropagation, which is discussed in @backprop, can be illustrated as a directed acyclic Graph with inter-connections. It contains a set of neurons distributed in different layers.
- Each neuron has a activation function.
- The first layer, shown on the left side in @nn-img, is called the input layer and has no predacessors. Additionally, is their input value the same as their output value.
- The last layer, shown on the right side in @nn-img, is called the output layer and have no successors. Their value represents the output of the Network
- All other neurons are grouped in the so called hidden layers. In @nn-img this is represented by the layer in the middle. A neural network can have an arbitrary amount of hidden layers. 
- The edges in the inter-connection graph, are the weights, which represent an arbitrary number in $RR$ and are updated during the trianing process.

#figure(image("images/Neural_Network_Illustration.png", width: 60%), 
caption: [Illustration of a Neural Network with 3 layers. Illustrated with @NNSVG])
<nn-img>

*Computation of the Output* 

The computation of the output in a #abbr.a[FNN] is referred to as a forward pass. Each neuron calculates its output by applying an activation function $f$ to sum of its inputs. For input neuron $i$, the aggregated input (also called the potential) is denoted as $xi_i$, yielding the expression $y = f(sum_i^n xi_i)$.

To complete a forward pass, this procedure is applied sequentially from the input layer through the hidden layers to the output layer. At each step, inputs are scaled by weight $w_(i j)$ before being summed and passed through the activation function. This process is captured by the following equations:

#math.equation(block: true, supplement: auto, 
$
    y_1 = f(sum_i^n w_(i j) x_i) "[input to hidden layer]"\
    y_(j+1) = f(sum_i^n w_(i j) y_j)  forall j in {1 dots k-1}"[hidden to hidden layer]"\
    o = f(w_(i j+1) y_j) "[hidden to output layer]"
$ 
)<forwardpass> @aggarwalNeuralNetworksDeep2023
where $j$ denotes the layer, ascending from input layer to output layer, $f$ activation function, $w_(i j)$ weight at index $i$ and layer $j$, $x$ as input at index $i$. The state of the neuron in the output layer $o$ can then by denoted as the output vector.


=== The Backpropagation Training Algorithm<backprop>
The backpropagation training algorithm is used to train all deep learning models described in this section. 

*Objective*: To identify a set of weights that guarantees that for every input vector, the output vector generated by the network is identical to (or sufficiently close to) the desired output vector.

*For a fixed and finite training set*:
The objective function represents the total error between the desired and actual outputs of all the output neurons for all the training patterns.

*Error Function*

#math.equation(
  block: true,
  $
    E = 1 /2 sum_p^P sum_i^N (y_(i p) - d_(i p))^2 
  $
)<errorfunction> @mrazovaMultilayeredNeuralNetworks

The error function measures how far the actual output is from the desired output.
Where $P$ is the number of training patterns, $N$ the number of output neurons, $d_(i p)$ is the desired output for pattern $p$, $y_(i p)$ the actual output of the neuron $i$ and output neuron $i$.

*Procedure*
#figure(
  box[
    #set align(left)
    1. Compute the actual output for the presented pattern
    2. Compare the actual output with the desired output
    3. Adjustment of weights and thresholds against the gradient of the error function (@errorfunction) for each layer from the output layer towards the input layer
  ], 
  caption: [Training Procedure of the backpropagation algorithm]
)

*Adjustment Rules*

#math.equation(
  block: true,
  $
    w_(i j)(t + 1) = w_(i j) + Delta_E w_(i j) (t) \
    Delta_E w_(i j) = - (partial E) / (partial w_(i j)) = - (partial E) / (partial y_j) (partial y_j) / (partial xi_j) (partial xi_j) / (partial w_(i j))
  $
)#cite(<mrazovaMultilayeredNeuralNetworks>)

where $Delta_E w_(i j)$ denotes the change of the Error Function with respect to $w_(i j)$, $E$ the Error Function, $y_j$ the output of the output neuron $j$, $xi_j$ the potential of the neuron $j$, $w_(i j)$ the weight with index $i$ at layer $j$ and $t$ to the timestep.

=== Recurrent Neural Networks
The feedforward #abbr.pla[FNN] discussed in @fnn are inherently limited to fixed-size, unordered input representations. This makes them unsuitable for sequential data, where both the order and length of the input can vary. To address this limitation, we introduce a class of models specifically designed to process variable-length sequences: #abbr.pla[RNN]. @fig:rnn shows a #abbr.a[RNN] for a input size of 1 and @fig:unrolled_rnn shows a unrolled #abbr.a[RNN] of size 4.

*Architecture*

A #abbr.a[RNN] consits of the following components:
- Input signal: The external data which is fed into the network at a timestep $t$ and represent the current information which the network is processing.
- State signal: Also known as the hidden state, represents the memory of the #abbr.a[RNN] for a given neuron. It contains information about the past inputs in the sequence and is updated at each time step based on the current input and the previous state. The hidden state is updated with the following formula: $h_t = f(h_(t-1), x_t)$. After the update, the hidden state of neuron $i$ serves as input into the neuron $i+1$ 
- Weights: The weights of the #abbr.a[RNN] neurons are shared among all different states. 
- Output: Each neuron has a output, which is denoted as $y_1$ - $y_4$ in @fig:unrolled_rnn. This output can serve as the output for the current state or as input into the next neuron. 
@schillingLecture05Sequential2025, @aggarwalNeuralNetworksDeep2023

 #grid(
  columns: 2, align: center,
  grid.cell([
    #figure(
      image("images/rnn_simple.png", height: 24%),
      caption: [#abbr.a[RNN]],
    )<fig:rnn>
  ]),
  grid.cell([
    #figure(
      image("images/rnn_unrolled.png", height: 24%),
      caption: [4 times unrolled #abbr.a[RNN]],
    )<fig:unrolled_rnn>
  ]),
)


*Vanishing Gradient Problem*

The #abbr.a[VGP] is a challenge encountered during the training of of #abbr.pla[RNN], particullary dealing with deep #abbr.pla[RNN] and long input sequences. It arieses from the way how gradients are updated during the backpropagation algorithm (discussed in @backprop), resulting in repeated multiplication of weight matrices and derivatives of activation functions across time steps. When these values are consistently smaller than one, the gradients exponentially decrease as they traverse earlier layers or time steps. Consequently, the gradients become vanishingly small, leading to negligible updates for earlier parameters and impairing the network's ability to learn long-range dependencies. The same principle aries, when the gradients become too large. In this case, the problem is called the exploding gradient problem.

@aggarwalNeuralNetworksDeep2023



=== Long Short Term Memory Neural Networks
#abbr.pla[LSTM] are a special form of #abbr.pla[RNN] designed to address the #abbr.a[VGP] while having a more fine-grained control over the previous input data and were introduced for the first time by  Sepp Hochreiter in 1997 @hochreiterLongShortTermMemory1997. They are an enhancement because the recurrence conditaions of how the hidden state $h_t$ is processed. To achieve this aim, we introduce a new hidden state of the same dimesion as $h_t$, which is called the cell state and is denoted as $c_t$. 
The key innovation of the #abbr.a[LSTM]  lies in its ability to control the flow of information using a set of gating mechanisms. These gates regulate how information is added to, removed from, or exposed from the cell state. Each gate is implemented as a sigmoid-activated neural layer and serves a distinct role in the update process.

*Architecture*

The internal structure of an #abbr.a[LSTM] cell is shown in @lstm-illustration. The figure illustrates how, at each time step $t$, the cell takes in the input vector $x_t$, the previous hidden state $h_(t-1)$, and the previous cell state $c_(t-1)$, and uses them to compute updated values for the current cell state $c_t$ and hidden state $h_t$.

@lstm-illustration shows an illustration of an #abbr.a[LSTM] Cell. 
#figure(
  image("images/LSTM_Cell_illustration.png"),
  caption: [
Schematic illustration of an #abbr.a[LSTM] cell highlighting the internal gating structure. The colored blocks represent the three core gates—Forget (blue), Input (green), and Output (red)—and show how they interact with the cell and hidden states to regulate information flow.
]
  )<lstm-illustration>

At each time step $t$ with a given input vector $x_t$, previous hidden state $h_(t-1)$ and previous cell state $c_(t-1)$, the #abbr.a[LSTM] performs the following computations:

- Forget Gate (shown in the blue part of @lstm-illustration): This gate decides which parts of the previous cell state should be forgotten. The value of the forget gate is calculated as: 
  - $f_t = sigma(w_f [h_(t-1), x_t]) + b_f) $
- Input Gate (shown in the green part of @lstm-illustration ): Decides which new information will be added to the cell state and is calculated as:
  - $i_t = sigma(w_i [h_(t-1), x_t] + b_i)$
- Output Gate (shown as the red part in @lstm-illustration): Determines which part of the cell state influences the hidden state and therefore the output. It is computed with: 
  - $o_t = sigma(w_o [h_(t-1), x_t] + b_o)$
- Candidate Cell State: Computes possible candidates $tilde(c_t)$ which can be added to the cell state, computed as: 
  - $tilde(c_t) = tanh(w_c [h_(t-1), x_t] + b_c)$
- Cell state update: Given the candidates, the cell state can be updated as:
  - $c_t = f_t dot c_(t-1) + i_t dot tilde(c_t)$
- Hidden state update:  The final hidden state is computed by applying the output gate to the activated cell state. It is computed with: 
  - $h_t = o_t dot tanh(c_t)$
Note: $w_x$ represents a complete weight matrix for each gate, $b_x$ denotes the bias for the corresponding gates, and $sigma$ denotes the sigmoid function.

@PyTorchFoundation, @thakurLSTMItsEquations2018  

=== Transformer
With the advent of Large Language Models and influential works such as Attention Is All You Need @vaswaniAttentionAllYou2023, Transformer architectures have gained significant traction in the field of Deep Learning. Originally developed for natural language processing tasks, Transformers have since been successfully adapted to a variety of domains, such as time series forcasting as shown by Q. Wen et. al. in  "Transformers in Time Series: A Survey" @wenTransformersTimeSeries2023 due to their ability to model long-range dependencies.

In the following section, the core components and mechanisms of the Transformer architecture are outlined. 
*Architecture*

An important concept in the Transformer architecture is Attention. It allows the model to capture dependencies between elements in the input sequence. An attention function can be viewed as a mapping from a query and a set of key–value pairs to an output. The output is a weighted sum of the values, where the weights are determined by a compatibility function between the query and the keys. This mechanism is illustrated in @self-attention-ill, where the input sequence is linearly projected into query, key, and value matrices to compute attention scores and generate contextualized representations. This procedure can be expressed with: $ "Attention"(Q,K, V) = "softmax"((Q K^T)/sqrt(d_k)) V $ @vaswaniAttentionAllYou2023 

When the dot product $Q K^T$ yields large values, the resulting attention scores can produce extremely sharp probability distributions after applying the softmax function. This can lead to vanishing gradients during training, making optimization unstable. T To mitigate this effect, the attention scores are scaled by a factor of $1/(sqrt(d_k))$, where $d_k$ is the dimensionality of the key vector. @vaswaniAttentionAllYou2023

In tasks involving sequential data, such as language modeling or time series forecasting, the model should not have access to future positions when making a prediction. To enforce this constraint, the Transformer uses a technique called masked attention, in which the attention weights for all positions beyond the current one are set to zero. This ensures that, when computing the representation for position $x_n$, the model can only attend to $x_(<=n)$ through $x_n$, but not to any $x_(>n)$.

#figure(image("images/self-attention.png",width: 60%),
caption: [
Self-attention mechanism illustrated with matrices. All matrices have shape $D dot N$, where $D$ is the sequence length and $N$ is the feature dimension. The input matrix is projected into three separate matrices: Queries ($Q$), Keys ($K$), and Values ($V$). The attention weights are computed by multiplying $Q$ with the transpose of $K$, followed by the Softmax function. The result is then used to weight the $V$ matrix, producing the final output as $"Softmax"(Q K^T) dot V$.
@princeUnderstandingDeepLearning
]
)<self-attention-ill>

While basic self-attention allows a model to compute contextual relationships between sequence elements, it operates in a single projection space, potentially limiting the diversity of information captured. To address this, Transformers employ Multi-Head Attention, a mechanism that enables the model to attend to information from multiple representation subspaces simultaneously, which was firstly described in "Attention is All You Need" @vaswaniAttentionAllYou2023
Instead of computing attention just once, the input sequence is projected into multiple sets of Queries, Keys, and Values using learned linear transformations—typically with smaller dimensionality $q < p$, where $p$ is the original embedding size. Each set of projections corresponds to a separate attention head, allowing the model to focus on different semantic or temporal aspects of the sequence.
These $n$ parallel attention heads independently compute attention outputs, which are then concatenated into a single vector of dimension $k dot q$. Since this dimensionality may differ from the original embedding size, the concatenated output is passed through a final linear projection layer (denoted $W^(text)$) to produce the final attention output. This structure is illustrated conceptually in @multi-head-attention, and it significantly enhances the expressiveness and robustness of the attention mechanism. This process can also be caluclated with: $ "MultiHead"(Q, K, V) = "Conact"("head"_1, dots, "head"_h)W^(O) \
"where " "head"_i = "Attention"(Q W_i^Q, K W_i ^K, V W_i ^V) $ @vaswaniAttentionAllYou2023


Multi-head attention not only improves model performance but also enables parallel computation of attention heads, which leads to efficient training, especially on modern hardware. @aggarwalNeuralNetworksDeep2023

#figure(image("images/multiheadattention.png"),
caption: [Multi-head attention mechanism. The input matrix $X$ of shape $D times N$, is linearly projected into multiple sets of Queries, Keys, and Values. Each set defines an individual attention head (e.g., Head 1, Head 2), which independently computes scaled dot-product attention. The outputs from all $H$ heads, each of size $D/H times N$, are then concatenated and projected through a final linear layer to produce the output matrix $O$ of shape $D times N$. @princeUnderstandingDeepLearning])<multi-head-attention>

*Embedding*

In tasks such as machine translation, input sequences composed of discrete tokens (e.g., words) must first be mapped to continuous vector representations through an embedding layer, which captures semantic information about each token. In contrast, time series data is inherently numerical and already exists in a continuous vector space. Nevertheless, to align the input dimensionality with the model's internal representation size (denoted as d_model in the Transformer architecture), we apply a linear transformation to project the raw input features into the desired embedding space.

Although this operation is technically a linear projection, it is commonly referred to as an "embedding" in the literature, including in the context of non-textual data, such as in the Vision Transformer (ViT) by Dosovitskiy et al. @dosovitskiyImageWorth16x162021. Following this convention, we refer to this input transformation layer as an embedding in our architecture as well.

*Positional Encoding*

While the self-attention mechanism is effective at capturing relationships between elements in a sequence, it lacks an inherent notion of order. Specifically, self-attention is permutation-equivariant, which means it produces the same output, regardless of the input sequences order. However, order is important when the input is a time series or a sentence.
To address this limitation, positional encodings are introduced to inject information about the position of each element in the input sequence. A common approach is to define a positional encoding matrix $P_i$ and add it to the original input matrix $X$. Each column of $P_i$ encodes a unique absolute position within the sequence, allowing the model to distinguish between inputs based not only on content but also on their order.

This encoding matrix $P_i$ can either be fixed e.g. using sinus function as it was done in "Attention is All You Need" @vaswaniAttentionAllYou2023 or learned during training. 
By learning absolute position information in this way, the Transformer model gains the ability to capture the sequential structure of the data, which is essential for tasks like damage forecasting or language understanding.

*Encoder Decoder*

Transformers are based on an encoder–decoder architecture, as illustrated in @encoder-decoder-architecture-ill. The encoder processes the full input sequence and generates a contextual representation, known as the encoder vector (shown in grey). The decoder then uses this representation to generate the output sequence token by token. Both components consist of stacked layers with a shared modular structure, including multi-head attention, feedforward sub-layers, residual connections, and normalization. 
#figure(
  image("images/encoder_decoder_architecture_illustration.png"),
  caption: [Abstracted illustration of the encoder–decoder architecture. The encoder receives an input sequence $x_1, dots, x_n$ and transforms it into a sequence of contextualized representations, here called the Encoder Vector, which is symbolically represented by two arrows to emphasize its role in guiding the decoding process. The encoder vector are passed to the decoder, which generates an output sequence $y_1, dots, y_k$, where the output length $k$ may differ from the input length $n$.@aggarwalNeuralNetworksDeep2023
]
)<encoder-decoder-architecture-ill>


During training, the encoder receives the full observed input sequence, such as past weather patterns over several weeks in the domain of weather forcasting. The decoder is provided with the leftmost portion of the target sequence, which is, the known values from the beginning of the forecast window. For example, if the goal is to predict the temperature over the next 10 days, the decoder might initially receive only a start-of-sequence token or the first known value and must predict the next value in the sequence. @aggarwalNeuralNetworksDeep2023


Although the original Transformer combines encoder and decoder modules, simplified variants such as BERT and GPT-3 omit either the decoder or encoder component. BERT uses an encoder-only architecture suited for classification and representation tasks, while GPT-3 is built on a decoder-only architecture optimized for generative tasks.

@frank-peter09_Transformers2025
@vaswaniAttentionAllYou2023, @princeUnderstandingDeepLearning



