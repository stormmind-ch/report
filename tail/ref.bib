@online{23Clustering,
  title = {2.3. {{Clustering}}},
  url = {https://scikit-learn/stable/modules/clustering.html},
  urldate = {2025-05-13},
  abstract = {Clustering of unlabeled data can be performed with the module sklearn.cluster. Each clustering algorithm comes in two variants: a class, that implements the fit method to learn the clusters on trai...},
  langid = {english},
  organization = {scikit-learn},
  file = {/Users/nilsgamperli/Zotero/storage/PJKLKPYK/clustering.html}
}

@misc{abdulkadirAustauschZurBachelorarbeit2025,
  title = {Austausch zur Bachelorarbeit},
  namea = {Abdulkadir, Ahmed},
  nameatype = {collaborator},
  date = {2025-09-04},
  url = {https://teams.microsoft.com/l/meetup-join/19%3ameeting_NTRjZmVkOTgtN2FlMC00MzMxLThlZGItZWFmYzgzNzFlNDIy%40thread.v2/0?context=%7b%22Tid%22%3a%225d1a9f9d-201f-4a10-b983-451cf65cbc1e%22%2c%22Oid%22%3a%22f9007b72-54cb-4536-a294-933ef964b39f%22%7d},
  langid = {ngerman}
}

@book{aggarwalNeuralNetworksDeep2023,
  title = {Neural {{Networks}} and {{Deep Learning}}: {{A Textbook}}},
  shorttitle = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Aggarwal, Charu C.},
  date = {2023},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-29642-0},
  url = {https://link.springer.com/10.1007/978-3-031-29642-0},
  urldate = {2025-04-07},
  isbn = {978-3-031-29641-3 978-3-031-29642-0},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/7QDX4IBM/Aggarwal - 2023 - Neural Networks and Deep Learning A Textbook.pdf}
}

@book{AmtlichesGemeindeverzeichnisSchweiz,
  title = {Amtliches Gemeindeverzeichnis der Schweiz - MS-Excel Version},
  number = {286080},
  publisher = {Bundesamt f√ºr Statistik (BFS) / BFS},
  url = {https://dam-api.bfs.admin.ch/hub/api/dam/assets/286080/master},
  urldate = {2025-02-25},
  isbn = {286080},
  langid = {DE/FR},
  file = {/Users/nilsgamperli/Zotero/storage/LGC52SZ2/be-b-00.04-agv-20050313.xls}
}

@online{bouthillierAccountingVarianceMachine2021,
  title = {Accounting for {{Variance}} in {{Machine Learning Benchmarks}}},
  author = {Bouthillier, Xavier and Delaunay, Pierre and Bronzi, Mirko and Trofimov, Assya and Nichyporuk, Brennan and Szeto, Justin and Sepah, Naz and Raff, Edward and Madan, Kanika and Voleti, Vikram and Kahou, Samira Ebrahimi and Michalski, Vincent and Serdyuk, Dmitriy and Arbel, Tal and Pal, Chris and Varoquaux, Ga√´l and Vincent, Pascal},
  date = {2021-03-01},
  eprint = {2103.03098},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.03098},
  url = {http://arxiv.org/abs/2103.03098},
  urldate = {2025-05-31},
  abstract = {Strong empirical evidence that one machine-learning algorithm A outperforms another one B ideally calls for multiple trials optimizing the learning pipeline over sources of variation such as data sampling, augmentation, parameter initialization, and hyperparameters choices. This is prohibitively expensive, and corners are cut to reach conclusions. We model the whole benchmarking process, revealing that variance due to data sampling, parameter initialization and hyperparameter choice impact markedly the results. We analyze the predominant comparison methods used today in the light of this variance. We show a counter-intuitive result that adding more sources of variation to an imperfect estimator approaches better the ideal estimator at a 51√ó reduction in compute cost. Building on these results, we study the error rate of detecting improvements, on five different deep-learning tasks/architectures. This study leads us to propose recommendations for performance comparisons.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nilsgamperli/Zotero/storage/Y8Y6N42I/Bouthillier et al. - 2021 - Accounting for Variance in Machine Learning Benchmarks.pdf}
}

@online{ChainResponsibility,
  title = {Chain of {{Responsibility}}},
  url = {https://refactoring.guru/design-patterns/chain-of-responsibility},
  urldate = {2025-06-04},
  abstract = {Chain of Responsibility is a behavioral design pattern that lets you pass requests along a chain of handlers. Upon receiving a request, each handler decides either to process the request or to pass it to the next handler in the chain.},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/NC65ZHWR/chain-of-responsibility.html}
}

@article{chawlaSMOTESyntheticMinority2002,
  title = {{{SMOTE}}: {{Synthetic Minority Over-sampling Technique}}},
  shorttitle = {{{SMOTE}}},
  author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
  date = {2002-06-01},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {jair},
  volume = {16},
  pages = {321--357},
  issn = {1076-9757},
  doi = {10.1613/jair.953},
  url = {https://www.jair.org/index.php/jair/article/view/10302},
  urldate = {2025-04-07},
  abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of ‚Äúnormal‚Äù examples with only a small percentage of ‚Äúabnormal‚Äù or ‚Äúinteresting‚Äù examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/HFRT5P7E/Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf}
}

@online{designerpartSchuettgutGewichtBigBag,
  title = {Sch√ºttgut-Gewicht - Big Bag Puhm | Alles √ºber Gewicht und Volumen},
  author = {Designerpart},
  url = {https://bigbag-puhm.at/handhabung/schuettgut-gewicht/},
  urldate = {2025-05-06},
  abstract = {Wie viel Gewicht kann ein Big Bag tragen? Erfahren Sie alles √ºber das Sch√ºttgut-Gewicht und wie Sie es optimal nutzen k√∂nnen.},
  langid = {ngerman},
  organization = {BigBag Puhm},
  file = {/Users/nilsgamperli/Zotero/storage/93ZXILMS/schuettgut-gewicht.html}
}

@online{DJLDeepJava,
  title = {{{DJL}} - {{Deep Java Library}}},
  url = {https://djl.ai/},
  urldate = {2025-06-03},
  file = {/Users/nilsgamperli/Zotero/storage/XWFMIMIF/djl.ai.html}
}

@dataset{dmg_xl,
  type = {excel},
  title = {USDB\_1972\_2023\_ohneSchadenszahlen\_ohneBeschriebe},
  author = {Swiss Federal Research Institute WSL},
  date = {2023},
  number = {Schadenszahlen},
  langid = {ngerman},
  file = {/Users/nilsgamperli/Zotero/storage/EVC9HQR4/USDB_1972_2023_ohneSchadenszahlen_ohneBeschriebe.xlsx}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2025-05-31},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nilsgamperli/Zotero/storage/3T8UW8Q6/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf}
}

@misc{duchaudPhoneCallHochbau2025,
  title = {Phone call Hochbau und Umwelt Affoltern am Albis},
  namea = {{Duchaud"}},
  nameatype = {collaborator},
  date = {2025-08-04},
  langid = {ngerman}
}

@article{eilanderGloballyApplicableFramework2023,
  title = {A Globally Applicable Framework for Compound Flood Hazard Modeling},
  author = {Eilander, Dirk and Couasnon, Ana√Øs and Leijnse, Tim and Ikeuchi, Hiroaki and Yamazaki, Dai and Muis, Sanne and Dullaart, Job and Haag, Arjen and Winsemius, Hessel C. and Ward, Philip J.},
  date = {2023-02-27},
  journaltitle = {Natural Hazards and Earth System Sciences},
  volume = {23},
  number = {2},
  pages = {823--846},
  publisher = {Copernicus GmbH},
  issn = {1561-8633},
  doi = {10.5194/nhess-23-823-2023},
  url = {https://nhess.copernicus.org/articles/23/823/2023/},
  urldate = {2025-04-21},
  abstract = {Coastal river deltas are susceptible to flooding from pluvial, fluvial, and coastal flood drivers. Compound floods, which result from the co-occurrence of two or more of these drivers, typically exacerbate impacts compared to floods from a single driver. While several global flood models have been developed, these do not account for compound flooding. Local-scale compound flood models provide state-of-the-art analyses but are hard to scale to other regions as these typically are based on local datasets. Hence, there is a need for globally applicable compound flood hazard modeling. We develop, validate, and apply a framework for compound flood hazard modeling that accounts for interactions between all drivers. It consists of the high-resolution 2D hydrodynamic Super-Fast INundation of CoastS (SFINCS) model, which is automatically set up from global datasets and coupled with a global hydrodynamic river routing model and a global surge and tide model. To test the framework, we simulate two historical compound flood events, Tropical Cyclone Idai and Tropical Cyclone Eloise in the Sofala province of Mozambique, and compare the simulated flood extents to satellite-derived extents on multiple days for both events. Compared to the global CaMa-Flood model, the globally applicable model generally performs better in terms of the critical success index (‚àí0.01‚Äì0.09) and hit rate (0.11‚Äì0.22) but worse in terms of the false-alarm ratio (0.04‚Äì0.14). Furthermore, the simulated flood depth maps are more realistic due to better floodplain connectivity and provide a more comprehensive picture as direct coastal flooding and pluvial flooding are simulated. Using the new framework, we determine the dominant flood drivers and transition zones between flood drivers. These vary significantly between both events because of differences in the magnitude of and time lag between the flood drivers. We argue that a wide range of plausible events should be investigated to obtain a robust understanding of compound flood interactions, which is important to understand for flood adaptation, preparedness, and response. As the model setup and coupling is automated, reproducible, and globally applicable, the presented framework is a promising step forward towards large-scale compound flood hazard modeling.},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/8DUL3E5A/Eilander et al. - 2023 - A globally applicable framework for compound flood hazard modeling.pdf}
}

@article{elmanFindingStructureTime1990,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L.},
  date = {1990-03},
  journaltitle = {Cognitive Science},
  shortjournal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  issn = {0364-0213, 1551-6709},
  doi = {10.1207/s15516709cog1402_1},
  url = {https://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1},
  urldate = {2025-05-13},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context‚Äêdependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/7244XPNB/Elman - 1990 - Finding Structure in Time.pdf}
}

@online{elsworthTimeSeriesForecasting2020,
  title = {Time {{Series Forecasting Using LSTM Networks}}: {{A Symbolic Approach}}},
  shorttitle = {Time {{Series Forecasting Using LSTM Networks}}},
  author = {Elsworth, Steven and G√ºttel, Stefan},
  date = {2020-03-12},
  eprint = {2003.05672},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.05672},
  url = {http://arxiv.org/abs/2003.05672},
  urldate = {2025-05-29},
  abstract = {Machine learning methods trained on raw numerical time series data exhibit fundamental limitations such as a high sensitivity to the hyper parameters and even to the initialization of random weights. A combination of a recurrent neural network with a dimension-reducing symbolic representation is proposed and applied for the purpose of time series forecasting. It is shown that the symbolic representation can help to alleviate some of the aforementioned problems and, in addition, might allow for faster training without sacrificing the forecast performance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nilsgamperli/Zotero/storage/B38EVZ5H/Elsworth and G√ºttel - 2020 - Time Series Forecasting Using LSTM Networks A Symbolic Approach.pdf;/Users/nilsgamperli/Zotero/storage/AYEZH8KG/2003.html}
}

@inreference{Erdrutsch2025,
  title = {Erdrutsch},
  booktitle = {Wikipedia},
  date = {2025-01-13T08:40:48Z},
  url = {https://de.wikipedia.org/w/index.php?title=Erdrutsch&oldid=252204629},
  urldate = {2025-05-04},
  abstract = {Ein Erdrutsch ist das Abgleiten gr√∂√üerer Erd- und Gesteinsmassen, meistens ausgel√∂st durch starke Niederschl√§ge (langandauernder Regen oder Starkregen) und das dadurch bedingte Eindringen von Wasser zwischen vorher gebundene Bodenschichten. Durch die Schwerkraft und die Verminderung der Haftreibung zwischen den Bodenschichten rutscht der Hang (bei ausreichend gro√üer Hangneigung) ab. Ein gro√üer Erdrutsch wird auch Bergrutsch genannt; wenn kleine Fl√§chen betroffen sind, auch Hangrutsch oder Hangrutschung. Ein Erdrutsch unterscheidet sich vom Bergsturz durch die geringere Geschwindigkeit.},
  langid = {ngerman},
  annotation = {Page Version ID: 252204629},
  file = {/Users/nilsgamperli/Zotero/storage/PLVCIDVH/Erdrutsch.html}
}

@letter{fachstellegisAREJIRAGIS2262EXTERN,
  type = {E-mail},
  title = {[{{ARE-JIRA}}] {{GIS-2262}} [{{EXTERN}}] {{Bodenbeschaffenheitskarte}}},
  author = {Fachstelle GIS},
  file = {/Users/nilsgamperli/Zotero/storage/U6UCPCA9/Fachstelle GIS - [ARE-JIRA] GIS-2262 [EXTERN] Bodenbeschaffenheitskarte.pdf}
}

@unpublished{frank-peter09_Transformers2025,
  title = {09\_{{Transformers}}},
  author = {Frank-Peter, Schilling},
  date = {2025-04-16},
  eventtitle = {Computer {{Vision}} with {{Deep Learning Lecture}}},
  langid = {english},
  venue = {Winterthur},
  file = {/Users/nilsgamperli/Zotero/storage/44RACN3J/09_Transformers.pdf}
}

@online{GeocodingAPIAPI,
  title = {Geocoding {{API}} - {{API Ninjas}}},
  url = {https://www.api-ninjas.com/api/geocoding},
  urldate = {2025-03-08},
  file = {/Users/nilsgamperli/Zotero/storage/LT94STBZ/geocoding.html}
}

@online{GeoHack00,
  title = {{{GeoHack}} (0; 0)},
  url = {https://geohack.toolforge.org/geohack.php?params=___N____E},
  urldate = {2025-03-07},
  file = {/Users/nilsgamperli/Zotero/storage/CBXUHRIX/geohack.html}
}

@online{GISBrowserGeoportalKanton,
  title = {{{GIS-Browser Geoportal Kanton Z√ºrich}}},
  url = {https://geo.zh.ch/maps?x=2693065&y=1253028&scale=279770&basemap=arelkbackgroundzh},
  urldate = {2025-05-06},
  file = {/Users/nilsgamperli/Zotero/storage/454W8TPD/maps.html}
}

@inproceedings{guptaHebbNetSimplifiedHebbian2021,
  title = {{{HebbNet}}: {{A Simplified Hebbian Learning Framework}} to Do {{Biologically Plausible Learning}}},
  shorttitle = {{{HebbNet}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gupta, Manas and Ambikapathi, ArulMurugan and Ramasamy, Savitha},
  date = {2021-06-06},
  pages = {3115--3119},
  publisher = {IEEE},
  location = {Toronto, ON, Canada},
  doi = {10.1109/ICASSP39728.2021.9414241},
  url = {https://ieeexplore.ieee.org/document/9414241/},
  urldate = {2024-12-17},
  abstract = {Backpropagation has revolutionized neural network training however, its biological plausibility remains questionable. Hebbian learning, a completely unsupervised and feedback free learning technique is a strong contender for a biologically plausible alternative. However, so far, it has neither achieved high accuracy performance vs. backprop, nor is the training procedure simple. In this work, we introduce a new Hebbian learning based neural network, called HebbNet. At the heart of HebbNet is an improved Hebbian approach that includes an updated activation threshold and gradient sparsity to the first principles of Hebbian learning. These enable an efficiently performing Hebbian approach with a simple training procedure. Further to this, the improved Hebbian rule also improves training dynamics by reducing the number of training epochs from 1500 to 200 and making training a one-step process from a two-step process. We also reduce heuristics by reducing hyper-parameters from 5 to 1, and number of search runs for hyper-parameter tuning from 12,600 to 13. Notwithstanding this, HebbNet still achieves strong test performance on MNIST and CIFAR-10 datasets vs. state-of-the-art.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-7281-7605-5},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/C63KLYG8/Gupta et al. - 2021 - HebbNet A Simplified Hebbian Learning Framework to do Biologically Plausible Learning.pdf}
}

@misc{hersbachERA5HourlyData2023,
  title = {{{ERA5}} Hourly Data on Single Levels from 1940 to Present},
  author = {Hersbach, H., Bell, B., Berrisford, P., Biavati, G., Hor√°nyi, A., Mu√±oz Sabater, J., Nicolas, J., Peubey, C., Radu, R., Rozum, I., Schepers, D., Simmons, A., Soci, C., Dee, D., Th√©paut, J-N.},
  date = {2023},
  doi = {10.24381/cds.adbb2d47},
  url = {https://cds.climate.copernicus.eu/doi/10.24381/cds.adbb2d47},
  organization = {ECMWF}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J√ºrgen},
  date = {1997-11-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
  urldate = {2025-05-26},
  abstract = {Learningtostoreinformationoverextendedtimeintervalsviarecurrentbackpropagation takesaverylongtime,mostlyduetoinsucient,decayingerrorbackow.Webrieyreview Hochreiter's1991analysisofthisproblem,thenaddressitbyintroducinganovel,ecient, gradient-basedmethodcalled\textbackslash LongShort-TermMemory"(LSTM).Truncatingthegradient wherethisdoesnotdoharm,LSTMcanlearntobridgeminimaltimelagsinexcessof1000 discretetimestepsbyenforcingconstanterrorowthrough\textbackslash constanterrorcarrousels"within specialunits.Multiplicativegateunitslearntoopenandcloseaccesstotheconstanterror ow.LSTMislocalinspaceandtime;itscomputationalcomplexitypertimestepandweight isO(1).Ourexperimentswitharticialdatainvolvelocal,distributed,real-valued,andnoisy patternrepresentations.IncomparisonswithRTRL,BPTT,RecurrentCascade-Correlation, Elmannets,andNeuralSequenceChunking,LSTMleadstomanymoresuccessfulruns,and learnsmuchfaster.LSTMalsosolvescomplex,articiallongtimelagtasksthathavenever beensolvedbypreviousrecurrentnetworkalgorithms.},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/FMY3BWRK/Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf}
}

@online{iraniPositionalEncodingTransformerBased2025,
  title = {Positional {{Encoding}} in {{Transformer-Based Time Series Models}}: {{A Survey}}},
  shorttitle = {Positional {{Encoding}} in {{Transformer-Based Time Series Models}}},
  author = {Irani, Habib and Metsis, Vangelis},
  date = {2025-02-17},
  eprint = {2502.12370},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.12370},
  url = {http://arxiv.org/abs/2502.12370},
  urldate = {2025-05-30},
  abstract = {Recent advancements in transformer-based models have greatly improved time series analysis, providing robust solutions for tasks such as forecasting, anomaly detection, and classification. A crucial element of these models is positional encoding, which allows transformers to capture the intrinsic sequential nature of time series data. This survey systematically examines existing techniques for positional encoding in transformer-based time series models. We investigate a variety of methods, including fixed, learnable, relative, and hybrid approaches, and evaluate their effectiveness in different time series classification tasks. Furthermore, we outline key challenges and suggest potential research directions to enhance positional encoding strategies. By delivering a comprehensive overview and quantitative benchmarking, this survey intends to assist researchers and practitioners in selecting and designing effective positional encoding methods for transformer-based time series models. The source code for the methods and experiments discussed in this survey is available on GitHub1.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nilsgamperli/Zotero/storage/UIY84F4H/Irani and Metsis - 2025 - Positional Encoding in Transformer-Based Time Series Models A Survey.pdf}
}

@article{kaurSystematicReviewImbalanced2020,
  title = {A {{Systematic Review}} on {{Imbalanced Data Challenges}} in {{Machine Learning}}: {{Applications}} and {{Solutions}}},
  shorttitle = {A {{Systematic Review}} on {{Imbalanced Data Challenges}} in {{Machine Learning}}},
  author = {Kaur, Harsurinder and Pannu, Husanbir Singh and Malhi, Avleen Kaur},
  date = {2020-07-31},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {52},
  number = {4},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3343440},
  url = {https://dl.acm.org/doi/10.1145/3343440},
  urldate = {2025-04-03},
  abstract = {In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.},
  langid = {english}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-30},
  eprint = {1412.6980},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2025-05-06},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nilsgamperli/Zotero/storage/MAJJEPUZ/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/nilsgamperli/Zotero/storage/43U4254A/1412.html}
}

@online{kongUnlockingPowerLSTM2025,
  title = {Unlocking the {{Power}} of {{LSTM}} for {{Long Term Time Series Forecasting}}},
  author = {Kong, Yaxuan and Wang, Zepu and Nie, Yuqi and Zhou, Tian and Zohren, Stefan and Liang, Yuxuan and Sun, Peng and Wen, Qingsong},
  date = {2025-02-24},
  eprint = {2408.10006},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.10006},
  url = {http://arxiv.org/abs/2408.10006},
  urldate = {2025-05-29},
  abstract = {Traditional recurrent neural network architectures, such as long short-term memory neural networks (LSTM), have historically held a prominent role in time series forecasting (TSF) tasks. While the recently introduced sLSTM for Natural Language Processing (NLP) introduces exponential gating and memory mixing that are beneficial for long term sequential learning, its potential short memory issue is a barrier to applying sLSTM directly in TSF. To address this, we propose a simple yet efficient algorithm named P-sLSTM, which is built upon sLSTM by incorporating patching and channel independence. These modifications substantially enhance sLSTM's performance in TSF, achieving state-of-the-art results. Furthermore, we provide theoretical justifications for our design, and conduct extensive comparative and analytical experiments to fully validate the efficiency and superior performance of our model.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nilsgamperli/Zotero/storage/D4JFEGAI/Kong et al. - 2025 - Unlocking the Power of LSTM for Long Term Time Series Forecasting.pdf;/Users/nilsgamperli/Zotero/storage/Z9A8ZAR6/2408.html}
}

@misc{liechtiAustauschUnwetterschaedenUnd2025,
  title = {Austausch zu Unwettersch√§den und Daten},
  namea = {Liechti, Katharina},
  nameatype = {collaborator},
  date = {2025-02-28},
  url = {https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZDk5ODM1MWQtN2E2Yi00NDZjLWFiNTEtOWE0ZWU4ODQ3YzA1%40thread.v2/0?context=%7b%22Tid%22%3a%225d1a9f9d-201f-4a10-b983-451cf65cbc1e%22%2c%22Oid%22%3a%225f85a69b-df7a-4a9a-acab-f5ad7d25b1a9%22%7d},
  langid = {ngerman}
}

@letter{liechtiREAnfrageZur2024,
  type = {E-mail},
  title = {{{RE}}: {{Anfrage}} Zur {{Nutzung}} Der {{Unwetterschadens-Datenbank}} F√ºr {{Bachelorarbeit}}},
  author = {Liechti, K√§thi},
  date = {2024-11-29}
}

@online{limTemporalFusionTransformers2020,
  title = {Temporal {{Fusion Transformers}} for {{Interpretable Multi-horizon Time Series Forecasting}}},
  author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
  date = {2020-09-27},
  eprint = {1912.09363},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1912.09363},
  url = {http://arxiv.org/abs/1912.09363},
  urldate = {2025-04-29},
  abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nilsgamperli/Zotero/storage/PNGFABV8/Lim et al. - 2020 - Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting.pdf;/Users/nilsgamperli/Zotero/storage/5ZJ6W6QG/1912.html}
}

@online{LoginOpenStackDashboard,
  title = {Login - {{OpenStack Dashboard}}},
  url = {https://apu.cloudlab.zhaw.ch/auth/login/?next=/},
  urldate = {2025-05-06},
  file = {/Users/nilsgamperli/Zotero/storage/932T52Y3/login.html}
}

@article{maissenThreestageModelPipeline2024,
  title = {A Three-Stage Model Pipeline Predicting Regional Avalanche Danger in {{Switzerland}} ({{RAvaFcast}} v1.0.0): A Decision-Support Tool for Operational Avalanche Forecasting},
  shorttitle = {A Three-Stage Model Pipeline Predicting Regional Avalanche Danger in {{Switzerland}} ({{RAvaFcast}} v1.0.0)},
  author = {Maissen, Alessandro and Techel, Frank and Volpi, Michele},
  date = {2024-10-30},
  journaltitle = {Geoscientific Model Development},
  volume = {17},
  number = {21},
  pages = {7569--7593},
  publisher = {Copernicus GmbH},
  issn = {1991-959X},
  doi = {10.5194/gmd-17-7569-2024},
  url = {https://gmd.copernicus.org/articles/17/7569/2024/},
  urldate = {2025-06-03},
  abstract = {Despite the increasing use of physical snow cover simulations in regional avalanche forecasting, avalanche forecasting is still an expert-based decision-making process. However, recently, it has become possible to obtain fully automated avalanche danger level predictions with satisfying accuracy by combining physically based snow cover models with machine learning approaches. These predictions are made at the location of automated weather stations close to avalanche starting zones. To bridge the gap between these local predictions and fully data- and model-driven regional avalanche danger maps, we developed and evaluated a three-stage model pipeline (RAvaFcast v1.0.0), involving the steps classification, interpolation, and aggregation. More specifically, we evaluated the impact of various terrain features on the performance of a Gaussian-process-based model for interpolation of local predictions to unobserved locations on a dense grid. Aggregating these predictions using an elevation-based strategy, we estimated the regional danger level and the corresponding elevation range for predefined warning regions, resulting in a forecast similar to the human-made public avalanche forecast in Switzerland. The best-performing model matched the human-made forecasts with a mean day accuracy of approximately 66 \% for the entire forecast domain and 70 \% specifically for the Alps. However, the performance depended strongly on the classifier's accuracy (i.e., a mean day accuracy of 68 \%) and the density of local predictions available for the interpolation task. Despite these limitations, we believe that the proposed three-stage model pipeline has the potential to improve the interpretability of machine-made danger level predictions and has, thus, the potential to assist avalanche forecasters during forecast preparation, for instance, by being integrated in the forecast process in the form of an independent virtual forecaster.},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/WZWU5FMZ/Maissen et al. - 2024 - A three-stage model pipeline predicting regional avalanche danger in Switzerland (RAvaFcast v1.0.0).pdf}
}

@online{MapsSwitzerlandSwiss,
  title = {Maps of {{Switzerland}} - {{Swiss Confederation}} - Map.Geo.Admin.Ch},
  url = {https://map.geo.admin.ch/#/map?lang=en&center=2660000,1190000&z=1&topic=ech&layers=ch.swisstopo.zeitreihen@year=1864,f;ch.bfs.gebaeude_wohnungs_register,f;ch.bav.haltestellen-oev,f;ch.swisstopo.swisstlm3d-wanderwege,f;ch.vbs.schiessanzeigen,f;ch.astra.wanderland-sperrungen_umleitungen,f&bgLayer=ch.swisstopo.pixelkarte-farbe},
  urldate = {2025-05-06},
  file = {/Users/nilsgamperli/Zotero/storage/PSNEH2XI/map.geo.admin.ch.html}
}

@online{mariaGewichtNasserErde2020,
  title = {Gewicht nasser Erde: Formel zum Umrechnen},
  shorttitle = {Gewicht nasser Erde},
  author = {Maria},
  date = {2020-08-04T13:39:00+00:00},
  url = {https://hortica.de/gewicht-nasse-erde/},
  urldate = {2025-05-06},
  abstract = {Sp√§testens wenn Sie Ihren Garten neu gestalten und eine gro√üe Menge Erde transportieren wollen, m√ºssen Sie wissen, wie viel nasse Erde wiegt.},
  langid = {ngerman},
  organization = {Hortica},
  file = {/Users/nilsgamperli/Zotero/storage/624MP5H5/gewicht-nasse-erde.html}
}

@book{martinCleanArchitectureCraftsmans2018,
  title = {Clean Architecture: A Craftsman's Guide to Software Structure and Design},
  shorttitle = {Clean Architecture},
  author = {Martin, Robert C. and Martin, Robert C.},
  date = {2018},
  series = {Robert {{C}}. {{Martin}} Series},
  publisher = {Prentice Hall},
  location = {London, England},
  abstract = {Building upon the success of best-sellers The Clean Coder and Clean Code, legendary software craftsman Robert C. "Uncle Bob" Martin shows how to bring greater professionalism and discipline to application architecture and design. As with his other books, Martin's Clean Architecture doesn't merely present multiple choices and options, and say "use your best judgment": it tells you what choices to make, and why those choices are critical to your success. Martin offers direct, no-nonsense answers to key architecture and design questions like: What are the best high level structures for different kinds of applications, including web, database, thick-client, console, and embedded apps? What are the core principles of software architecture? What is the role of the architect, and what is he/she really trying to achieve? What are the core principles of software design? How do designs and architectures go wrong, and what can you do about it? What are the disciplines and practices of professional architects and designers? Clean Architecture is essential reading for every software architect, systems analyst, system designer, and software manager -- and for any programmer who aspires to these roles or is impacted by their work},
  isbn = {978-0-13-449416-6},
  langid = {english},
  pagetotal = {404},
  keywords = {Computer programming,Computer software,COMPUTERS / Computer Architecture,Development,History,Software architecture},
  annotation = {OCLC: on1004983973},
  file = {/Users/nilsgamperli/Zotero/storage/QS8MX6NU/Martin and Martin - 2018 - Clean architecture a craftsman's guide to software structure and design.pdf}
}

@online{MeteoSwissIDAWEBLogin,
  title = {{{MeteoSwiss IDAWEB}}: {{Login}} at {{IDAWEB}}},
  url = {https://gate.meteoswiss.ch/idaweb/login.do},
  urldate = {2025-05-06},
  file = {/Users/nilsgamperli/Zotero/storage/D93XBZ9S/login.html}
}

@unpublished{mrazovaMultilayeredNeuralNetworks,
  title = {Multi-Layered {{Neural Networks}}},
  author = {Mr√°zov√°, Iveta},
  eventtitle = {Neural {{Networks NAIL069}}},
  langid = {english},
  venue = {Charles University, Prague},
  file = {/Users/nilsgamperli/Zotero/storage/HZN4VPKU/NN-lecture_24-Multi-Layered_NN_F.pdf}
}

@misc{muellerPhoneCallAmt2025,
  title = {phone call Amt f√ºr Raum Entwicklung und Vermessung},
  namea = {M√ºller},
  nameatype = {collaborator},
  date = {2025-08-04},
  langid = {ngerman}
}

@online{NNSVG,
  title = {{{NN SVG}}},
  url = {http://alexlenail.me/NN-SVG/},
  urldate = {2025-05-03},
  file = {/Users/nilsgamperli/Zotero/storage/4C882D58/NN-SVG.html}
}

@online{OpenCageEasyOpen,
  title = {{{OpenCage}} - {{Easy}}, {{Open}}, {{Worldwide}}, {{Affordable Geocoding}} and {{Geosearch}}},
  url = {https://opencagedata.com/},
  urldate = {2025-05-06},
  abstract = {An easy-to-use forward and reverse geocoding API. Worldwide coverage. Affordable, predictable pricing. Open data.},
  langid = {english}
}

@online{OpenMeteocom,
  title = {üëã {{About}} | {{Open-Meteo}}.Com},
  url = {https://open-meteo.com/en/about},
  urldate = {2025-06-03},
  file = {/Users/nilsgamperli/Zotero/storage/249GFPB7/about.html}
}

@misc{PhoneCallHochbau2025,
  title = {phone call Hochbau Amt Z√ºrich},
  date = {2025-08-04},
  langid = {ngerman}
}

@article{princeUnderstandingDeepLearning,
  title = {Understanding {{Deep Learning}}},
  author = {Prince, Simon J D},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/ZXSG4QF4/Prince - Understanding Deep Learning.pdf}
}

@online{PyTorchFoundation,
  title = {{{PyTorch Foundation}}},
  url = {https://pytorch.org/},
  urldate = {2025-05-06},
  abstract = {PyTorch Foundation is the deep learning community home for the open source PyTorch framework and ecosystem.},
  langid = {american},
  organization = {PyTorch},
  file = {/Users/nilsgamperli/Zotero/storage/I4G9IY43/pytorch.org.html}
}

@online{RAvaFcastKunstlicheIntelligenz,
  title = {{{RAvaFcast}} - {{Eine K√ºnstliche Intelligenz}} Zur {{Vorhersage}} Der Regionalen {{Lawinen-Gefahrenstufe}} in Der {{Schweiz}} - Opendata.Swiss},
  url = {https://opendata.swiss/en/showcase/ravafcast-eine-kunstliche-intelligenz-zur-vorhersage-der-regionalen-lawinen-gefahrenstufe-in-de},
  urldate = {2025-06-03},
  abstract = {\_Deutsch\_ RAvaFcast ist ein dreistufiges Modell des Maschinellen Lernens zur Vorhersage der regionalen Lawinengefahrenstufe. Im ersten Schritt werden die Gefahrenstufen an den Standorten der...},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/PJBTYNEY/ravafcast-eine-kunstliche-intelligenz-zur-vorhersage-der-regionalen-lawinen-gefahrenstufe-in-de.html}
}

@online{RecipeTrainingNeural,
  title = {A {{Recipe}} for {{Training Neural Networks}}},
  url = {http://karpathy.github.io/2019/04/25/recipe/},
  urldate = {2025-03-12},
  file = {/Users/nilsgamperli/Zotero/storage/6UACH873/recipe.html}
}

@online{RecipeTrainingNeurala,
  title = {A {{Recipe}} for {{Training Neural Networks}}},
  url = {http://karpathy.github.io/2019/04/25/recipe/},
  urldate = {2025-03-12},
  file = {/Users/nilsgamperli/Zotero/storage/CTCP6DZH/recipe.html}
}

@unpublished{schillingLecture05Sequential2025,
  title = {Lecture 05: {{Sequential Models}}},
  author = {Schilling, Frank-Peter and Cai, Zhaw},
  date = {2025-03-19},
  langid = {english},
  venue = {ZHAW School of Engineering},
  file = {/Users/nilsgamperli/Zotero/storage/G4M3RAG4/Schilling and Cai - Lecture 05 Sequential Models.pdf}
}

@online{ScikitlearnMachineLearning,
  title = {Scikit-Learn: Machine Learning in {{Python}} ‚Äî Scikit-Learn 1.6.1 Documentation},
  url = {https://scikit-learn.org/stable/#},
  urldate = {2025-05-06},
  file = {/Users/nilsgamperli/Zotero/storage/MSSC79Z8/stable.html}
}

@article{sherstinskyFundamentalsRecurrentNeural2020,
  title = {Fundamentals of {{Recurrent Neural Network}} ({{RNN}}) and {{Long Short-Term Memory}} ({{LSTM}}) Network},
  author = {Sherstinsky, Alex},
  date = {2020-03},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {404},
  pages = {132306},
  issn = {01672789},
  doi = {10.1016/j.physd.2019.132306},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167278919305974},
  urldate = {2025-04-16},
  abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of ‚Äò‚Äòunrolling‚Äô‚Äô an RNN is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the ‚Äò‚ÄòVanilla LSTM‚Äô‚Äô1 network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/C7HMH6Q9/Sherstinsky - 2020 - Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network.pdf}
}

@online{shrivastavaCrossValidationTime2020,
  title = {Cross {{Validation}} in {{Time Series}}},
  author = {Shrivastava, Soumya},
  date = {2020-01-17T05:52:39},
  url = {https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4},
  urldate = {2025-04-24},
  abstract = {Cross Validation:},
  langid = {english},
  organization = {Medium},
  file = {/Users/nilsgamperli/Zotero/storage/PYBN94DW/cross-validation-in-time-series-566ae4981ce4.html}
}

@inreference{StandardScore2025,
  title = {Standard Score},
  booktitle = {Wikipedia},
  date = {2025-05-24T16:39:33Z},
  url = {https://en.wikipedia.org/w/index.php?title=Standard_score&oldid=1291997598},
  urldate = {2025-05-29},
  abstract = {In statistics, the standard score or z-score is the number of standard deviations by which the value of a raw score (i.e., an observed value or data point) is above or below the mean value of what is being observed or measured. Raw scores above the mean have positive standard scores, while those below the mean have negative standard scores.  It is calculated by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This process of converting a raw score into a standard score is called standardizing or normalizing (however, "normalizing" can refer to many types of ratios; see Normalization for more).  Standard scores are most commonly called z-scores; the two terms may be used interchangeably, as they are in this article. Other equivalent terms in use include z-value, z-statistic, normal score, standardized variable and pull in high energy physics.  Computing a z-score requires knowledge of the mean and standard deviation of the complete population to which a data point belongs; if one only has a sample of observations from the population, then the analogous computation using the sample mean and sample standard deviation yields the t-statistic.},
  langid = {english},
  annotation = {Page Version ID: 1291997598},
  file = {/Users/nilsgamperli/Zotero/storage/NYUMRR8Z/Standard_score.html}
}

@article{sunDevelopmentLSTMBroadcasting2022,
  title = {Development of an {{LSTM}} Broadcasting Deep-Learning Framework for Regional Air Pollution Forecast Improvement},
  author = {Sun, Haochen and Fung, Jimmy C. H. and Chen, Yiang and Li, Zhenning and Yuan, Dehao and Chen, Wanying and Lu, Xingcheng},
  date = {2022-11-21},
  journaltitle = {Geoscientific Model Development},
  volume = {15},
  number = {22},
  pages = {8439--8452},
  publisher = {Copernicus GmbH},
  issn = {1991-959X},
  doi = {10.5194/gmd-15-8439-2022},
  url = {https://gmd.copernicus.org/articles/15/8439/2022/},
  urldate = {2025-04-29},
  abstract = {Deep-learning frameworks can effectively forecast the air pollution data for individual stations by decoding time series data. However, most of the existing time-series-based deep-learning models use offline spatial interpolation strategies and thus cannot reliably project the station-based forecast to the spatial region of interest. In this study, the station-based long short-term memory (LSTM) technique was extended for spatial air quality forecasting by combining a novel deep-learning layer, termed the broadcasting layer, which incorporates a learnable weight decay parameter designed for point-to-area extension. Unlike most existing deep-learning-based methods that isolate the interpolation from the model training process, the proposed end-to-end LSTM broadcasting framework can consider the temporal characteristics of the time series and spatial relationships among different stations. To validate the proposed deep-learning framework, PM2.5 and O3 forecasts for the next 48 h were obtained using 3D chemical transport model simulation results and ground observation data as the inputs. The root mean square error associated with the proposed framework was 40 \% and 20 \% lower than those of the Weather Research and Forecasting‚ÄìCommunity Multiscale Air Quality model and an offline combination of the deep-learning and spatial interpolation methods, respectively. The novel LSTM broadcasting framework can be extended for air pollution forecasting in other regions of interest.},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/GP8NJSI8/Sun et al. - 2022 - Development of an LSTM broadcasting deep-learning framework for regional air pollution forecast impr.pdf}
}

@online{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  date = {2014-12-14},
  eprint = {1409.3215},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.3215},
  url = {http://arxiv.org/abs/1409.3215},
  urldate = {2025-04-22},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT‚Äô14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM‚Äôs BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM‚Äôs performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nilsgamperli/Zotero/storage/M7GEUZXP/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf}
}

@misc{swissfederalresearchinstitutewslInfos_Daten_Unwetterschadensdatenbank_WSL_english2023,
  title = {Infos\_{{Daten}}\_{{Unwetterschadensdatenbank}}\_{{WSL}}\_english},
  shorttitle = {{{WSL-Disclaimer}}},
  author = {Swiss Federal Research Institute WSL},
  date = {2023},
  url = {Email from K. Liechti},
  urldate = {2024-11-29},
  langid = {english},
  organization = {Swiss Federal Research Institute WSL},
  file = {/Users/nilsgamperli/Zotero/storage/92G9GY62/Infos_Daten_Unwetterschadensdatenbank_WSL_english.pdf}
}

@online{SwissReRapid2025,
  title = {Swiss {{Re Rapid Damage Assessment}} | {{Swiss Re}}},
  date = {2025-02-07},
  url = {https://www.swissre.com/reinsurance/property-and-casualty/solutions/property-solutions/rapid-damage-assessment.html},
  urldate = {2025-06-03},
  abstract = {Leverage Swiss Re‚Äôs Rapid Damage Assessment platform to streamline end-to-end NatCat claims processing. Discover its powerful features here.},
  langid = {english},
  file = {/Users/nilsgamperli/Zotero/storage/L75AH362/rapid-damage-assessment.html}
}

@online{systemadmin_umweltHochwasserWieSie2012,
  type = {Text},
  title = {Hochwasser ‚Äì wie sie entstehen und wie der Mensch sie beeinflusst},
  author = {Systemadmin\_Umwelt},
  date = {2012-06-13T12:00+02:00},
  publisher = {Umweltbundesamt},
  url = {https://www.umweltbundesamt.de/themen/wasser/extremereignisse/hochwasser},
  urldate = {2025-05-04},
  abstract = {Hochwasser sind nat√ºrliche Ereignisse, sie treten regelm√§√üig auf und sind fester Bestandteil des Abflussgeschehens. Die Entstehung von Hochwasser h√§ngt von der St√§rke der Niederschl√§ge, den Eigenschaften des Einzugsgebietes und vom Fluss ab. Der Mensch kann Hochwasser, ihren Verlauf und die Auswirkungen beeinflussen und verst√§rken.},
  langid = {ngerman},
  organization = {Umweltbundesamt},
  file = {/Users/nilsgamperli/Zotero/storage/9DQUH5J5/hochwasser.html}
}

@online{thakurLSTMItsEquations2018,
  title = {{{LSTM}} and Its Equations},
  author = {Thakur, Divyanshu},
  date = {2018-07-06T09:44:04},
  url = {https://medium.com/@divyanshu132/lstm-and-its-equations-5ee9246d04af},
  urldate = {2025-05-05},
  abstract = {LSTM stands for Long Short Term Memory, I myself found it difficult to directly understand LSTM without any prior knowledge of the Gates‚Ä¶},
  langid = {english},
  organization = {Medium},
  file = {/Users/nilsgamperli/Zotero/storage/D27EBK8B/lstm-and-its-equations-5ee9246d04af.html}
}

@letter{ueltschiBodenbeschaffenheitskarte,
  type = {E-mail},
  title = {Bodenbeschaffenheitskarte},
  author = {Ueltschi, Damian},
  file = {/Users/nilsgamperli/Zotero/storage/CEVUFKLJ/Ueltschi - Bodenbeschaffenheitskarte.pdf}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-05-26},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nilsgamperli/Zotero/storage/XRHWBWA7/Vaswani et al. - 2023 - Attention Is All You Need.pdf}
}

@online{VergrabenUndVergessen2019,
  title = {Vergraben und vergessen},
  date = {2019-04-02},
  url = {https://www.derbund.ch/vergraben-und-vergessen-413137488243},
  urldate = {2025-05-06},
  abstract = {Seit Jahrzehnten sucht der Bund Endlager f√ºr radioaktive Abf√§lle. In B√ºlach ZH f√§hrt jetzt erstmals ein Bohrer auf ‚Äì ohne nennenswerten Widerstand.},
  langid = {ngerman},
  organization = {Der Bund},
  file = {/Users/nilsgamperli/Zotero/storage/5B3VX5W8/vergraben-und-vergessen-413137488243.html}
}

@online{WeightsBiases,
  title = {Weights \& {{Biases}}},
  url = {httpss://wandb.ai/wandb_fc/articles/reports/What-Is-Bayesian-Hyperparameter-Optimization-With-Tutorial---Vmlldzo1NDQyNzcw},
  urldate = {2025-05-31},
  abstract = {Weights \& Biases, developer tools for machine learning},
  langid = {english},
  organization = {W\&B},
  file = {/Users/nilsgamperli/Zotero/storage/5Q3QFAS6/What-Is-Bayesian-Hyperparameter-Optimization-With-Tutorial---Vmlldzo1NDQyNzcw.html}
}

@online{wenTransformersTimeSeries2023,
  title = {Transformers in {{Time Series}}: {{A Survey}}},
  shorttitle = {Transformers in {{Time Series}}},
  author = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  date = {2023-05-11},
  eprint = {2202.07125},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.07125},
  url = {http://arxiv.org/abs/2202.07125},
  urldate = {2025-05-26},
  abstract = {Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance. A corresponding resource that has been continuously updated can be found in the GitHub repository1.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/Users/nilsgamperli/Zotero/storage/9NVTQE4F/Wen et al. - 2023 - Transformers in Time Series A Survey.pdf}
}

@online{WieEntstehtHochwasser,
  title = {Wie Entsteht {{Hochwasser}}? | {{Nds}}. {{Landesbetrieb}} F√ºr {{Wasserwirtschaft}}, {{K√ºsten-}} Und {{Naturschutz}}},
  url = {https://www.nlwkn.niedersachsen.de/hochwasserschutz/hintergrundinformationen/wie_entsteht_hochwasser/fachliche-grundlagen-wie-entsteht-hochwasser-119741.html},
  urldate = {2025-05-04},
  file = {/Users/nilsgamperli/Zotero/storage/TMNHR3RI/fachliche-grundlagen-wie-entsteht-hochwasser-119741.html}
}

@online{yadavPyTorchEmbeddingLayer2025,
  title = {{{PyTorch Embedding Layer}} for {{Categorical Data}}},
  author = {Yadav, Amit},
  date = {2025-04-18T00:31:11},
  url = {https://medium.com/biased-algorithms/pytorch-embedding-layer-for-categorical-data-096af5757353},
  urldate = {2025-04-29},
  abstract = {‚ÄúIf you can‚Äôt explain it simply, you don‚Äôt understand it well enough.‚Äù{$\mkern1mu$}‚Äî{$\mkern1mu$}Albert Einstein.},
  langid = {english},
  organization = {Biased-Algorithms},
  file = {/Users/nilsgamperli/Zotero/storage/W7H8DYDI/pytorch-embedding-layer-for-categorical-data-096af5757353.html}
}

@article{yuSpatialTemporalCombination2024,
  title = {Spatial‚ÄìTemporal Combination and Multi-Head Flow-Attention Network for Traffic Flow Prediction},
  author = {Yu, Lianfei and Liu, Wenbo and Wu, Dong and Xie, Dongmei and Cai, Chuang and Qu, Zhijian and Li, Panjing},
  date = {2024-04-26},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {14},
  number = {1},
  pages = {9604},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-60337-7},
  url = {https://www.nature.com/articles/s41598-024-60337-7},
  urldate = {2025-04-29},
  abstract = {Traffic flow prediction based on spatial‚Äìtemporal data plays a vital role in traffic management. However, it still faces serious challenges due to the complex spatial‚Äìtemporal correlation in nonlinear spatial‚Äìtemporal data. Some previous methods have limited ability to capture spatial‚Äìtemporal correlation, and ignore the quadratic complexity problem in the traditional attention mechanism. To this end, we propose a novel spatial‚Äìtemporal combination and multi-head flow-attention network (STCMFA) to model the spatial‚Äìtemporal correlation in road networks. Firstly, we design a temporal sequence multi-head flow attention (TS-MFA), in which the unique source competition mechanism and sink allocation mechanism make the model avoid attention degradation without being affected by inductive biases. Secondly, we use GRU instead of the linear layer in traditional attention to map the input sequence, which further enhances the temporal modeling ability of the model. Finally, we combine the GCN with the TS-MFA module to capture the spatial‚Äìtemporal correlation, and introduce residual mechanism and feature aggregation strategy to further improve the performance of STCMFA. Extensive experiments on four real-world traffic datasets show that our model has excellent performance and is always significantly better than other baselines.},
  langid = {english},
  keywords = {Computer science,Information technology},
  file = {/Users/nilsgamperli/Zotero/storage/JBSRN592/Yu et al. - 2024 - Spatial‚Äìtemporal combination and multi-head flow-attention network for traffic flow prediction.pdf}
}

@software{zippenfenigOpenMeteocomWeatherAPI2023,
  title = {Open-{{Meteo}}.Com {{Weather API}}},
  author = {Zippenfenig, Patrick},
  date = {2023},
  doi = {10.5281/zenodo.7970649},
  url = {https://open-meteo.com/}
}

@letter{zippenfenigProfessionalAPIKey,
  type = {E-mail},
  title = {Professional {{API Key}} for {{Research}} Purposes},
  author = {Zippenfenig, Patrick},
  file = {/Users/nilsgamperli/Zotero/storage/WJBPT96S/Zippenfenig - Professional API Key for Research purposes.pdf}
}

@letter{zotero-item-247,
  type = {E-mail}
}
